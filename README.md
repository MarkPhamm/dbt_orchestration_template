# dbt Orchestration Template

A production-ready template for orchestrating data transformations on Snowflake using Apache Airflow with Astronomer. This template provides a scalable and maintainable foundation for building automated data pipelines in a modern cloud data stack.

## ğŸš€ Features

- **Snowflake Support**: Optimized for Snowflake data warehouse
- **Airflow Orchestration**: Powered by Apache Airflow with Astronomer
- **Containerized Environment**: Docker-based development and deployment
- **Development Tools**: Includes Jupyter notebooks for development and testing
- **Modular Structure**: Organized project structure for easy maintenance

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data/                          # Data files and sample datasets
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for development
â”œâ”€â”€ orchestration/                 # Airflow orchestration setup
â”‚   â”œâ”€â”€ dags/                      # Airflow DAG definitions
â”‚   â”‚   â”œâ”€â”€ dbt/                   # dbt project files and models
â”‚   â”‚   â”œâ”€â”€ transformation_dag.py  # Main DAG for dbt transformations
â”‚   â”‚   â””â”€â”€ .airflowignore
â”‚   â”œâ”€â”€ plugins/                   # Custom Airflow plugins
â”‚   â”œâ”€â”€ include/                   # Shared resources
â”‚   â”œâ”€â”€ tests/                     # Airflow DAG tests
â”‚   â”œâ”€â”€ .astro/                    # Astronomer configuration
â”‚   â”œâ”€â”€ Dockerfile                 # Container configuration
â”‚   â”œâ”€â”€ requirements.txt           # Python dependencies
â”‚   â”œâ”€â”€ packages.txt               # System packages
â”‚   â”œâ”€â”€ airflow_settings.yaml      # Airflow configuration
â”‚   â”œâ”€â”€ .env                       # Environment variables
â”‚   â””â”€â”€ .dockerignore              # Docker ignore rules
â””â”€â”€ requirements.txt               # Main project dependencies

```

## ğŸ› ï¸ Prerequisites

- Python 3.8+
- Docker and Docker Compose
- Snowflake account
- Astronomer CLI

## âš™ï¸ Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd dbt_orchestration_template
   ```

2. **Set up Python environment**
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. **Configure environment variables**
   - Copy `.env.example` to `.env`
   - Update the variables with your Snowflake credentials and other configurations

4. **Set up Airflow with Astronomer**
   ```bash
   cd orchestration
   
   # Starting the project
   astro dev start --wait 5m
   
   # When finished working
   astro dev stop
   ```

5. **Set up dbt project**
   ```bash
   cd orchestration/dags/dbt
   dbt init # then set up dbt with <your_dbt_project_name>
   ```

6. **Configure Transformation DAG**
   - Navigate to `orchestration/dags/transformation_dag.py`
   - Update the following parameters:
     ```python
     # Update these values in the DbtDag configuration
     your_project_name = "<your_dbt_project_name>"  # Your dbt project name
     dbt_profile_name = "<your_dbt_profile_name>" # Your dbt profile name
     dag_id = "<your_dag_id>"     # Your preferred DAG ID
     ```

7. **Set up Snowflake Connection in Airflow**
   - Open Airflow UI at `http://localhost:8080`
   - Go to Admin â†’ Connections
   - Click "Add a new record" â†’ name it `snowflake_conn`
   - Fill in the connection details:
     - Connection Id: `snowflake_conn`
     - Connection Type: `Snowflake`
     - Host: `your-account.snowflakecomputing.com`
     - Schema: `your_schema`
     - Login: `your_username`
     - Password: `your_password`
     - Account: `your_account`
     - Database: `your_database`
     - Warehouse: `your_warehouse`
     - Role: `your_role`

## ğŸš€ Usage

1. **Start the development environment**
   ```bash
   cd orchestration
   
   # Starting the project
   astro dev start --wait 5m
   
   # When finished working
   astro dev stop
   ```

2. **Access Airflow UI**
   - Open `http://localhost:8080` in your browser
   - Default credentials: admin/admin

3. **Work with dbt**
   ```bash
   cd orchestration/dags/<your_dbt_project_name>
   dbt run
   ```

## ğŸ”§ Development

- Use Jupyter notebooks in the `notebooks/` directory for development and testing
- Add new DAGs in the `orchestration/dags/` directory
- Develop dbt models within the `orchestration/dags/dbt/<your_dbt_project_name>` directory

## ğŸ“š Documentation

- [Airflow Documentation](https://airflow.apache.org/docs/)
- [Astronomer Documentation](https://docs.astronomer.io/)
- [Snowflake Documentation](https://docs.snowflake.com/)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. 